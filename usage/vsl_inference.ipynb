{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7647f6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\pc\\anaconda3\\envs\\retro\\lib\\site-packages (from imageio) (1.23.5)\n",
      "Requirement already satisfied: pillow>=8.3.2 in c:\\users\\pc\\anaconda3\\envs\\retro\\lib\\site-packages (from imageio) (10.4.0)\n",
      "Downloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.35.1\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aec68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_2688\\2782762199.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Could not find a backend to open `<video0>`` with iomode `r`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     51\u001b[0m last_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# keep last prediction\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Webcam capture with imageio\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m iio\u001b[38;5;241m.\u001b[39mimiter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<video0>\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# \"<video0>\" is default webcam\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(frame)  \u001b[38;5;66;03m# already RGB\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     res \u001b[38;5;241m=\u001b[39m mp_hands\u001b[38;5;241m.\u001b[39mprocess(rgb)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\retro\\lib\\site-packages\\imageio\\v3.py:90\u001b[0m, in \u001b[0;36mimiter\u001b[1;34m(uri, plugin, extension, format_hint, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimiter\u001b[39m(uri, \u001b[38;5;241m*\u001b[39m, plugin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, format_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read a sequence of ndimages from a URI.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Returns an iterable that yields ndimages from the given URI. The exact\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlegacy_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m img_file\u001b[38;5;241m.\u001b[39miter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     99\u001b[0m             \u001b[38;5;66;03m# Note: casting to ndarray here to ensure compatibility\u001b[39;00m\n\u001b[0;32m    100\u001b[0m             \u001b[38;5;66;03m# with the v2.9 API\u001b[39;00m\n\u001b[0;32m    101\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(image)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\envs\\retro\\lib\\site-packages\\imageio\\core\\imopen.py:281\u001b[0m, in \u001b[0;36mimopen\u001b[1;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the extension, the following plugins might add capable backends:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_candidates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         )\n\u001b[0;32m    280\u001b[0m request\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m--> 281\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err_type(err_msg)\n",
      "\u001b[1;31mOSError\u001b[0m: Could not find a backend to open `<video0>`` with iomode `r`."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v3 as iio\n",
    "\n",
    "# -------------------------\n",
    "# Model definition\n",
    "# -------------------------\n",
    "class SignLSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=126):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Load trained model\n",
    "# -------------------------\n",
    "classes = [\"cam_on\", \"toi\"]  # same as training\n",
    "model = SignLSTM(num_classes=2)\n",
    "model.load_state_dict(torch.load(\n",
    "    r\"D:\\WORK\\Python\\Project\\vsl_mediapipe\\models\\vsl_model_v2.pth\",\n",
    "    map_location=\"cpu\"\n",
    "))\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# Mediapipe setup\n",
    "# -------------------------\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# -------------------------\n",
    "# Buffer for sequences\n",
    "# -------------------------\n",
    "seq_len = 30\n",
    "buffer = deque(maxlen=seq_len)\n",
    "last_pred = None  # keep last prediction\n",
    "\n",
    "# -------------------------\n",
    "# Webcam capture with imageio\n",
    "# -------------------------\n",
    "for frame in iio.imiter(\"<video0>\"):  # \"<video0>\" is default webcam\n",
    "    rgb = np.ascontiguousarray(frame)  # already RGB\n",
    "    res = mp_hands.process(rgb)\n",
    "\n",
    "    detected = False\n",
    "    left_hand = [0] * 63\n",
    "    right_hand = [0] * 63\n",
    "\n",
    "    # -------------------------\n",
    "    # Hand landmark detection\n",
    "    # -------------------------\n",
    "    if res.multi_hand_landmarks and res.multi_handedness:\n",
    "        detected = True\n",
    "        for hand_landmarks, handedness in zip(res.multi_hand_landmarks, res.multi_handedness):\n",
    "            pts = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                pts.extend([lm.x, lm.y, lm.z])\n",
    "            label = handedness.classification[0].label\n",
    "            if label == \"Left\":\n",
    "                left_hand = pts\n",
    "            else:\n",
    "                right_hand = pts\n",
    "\n",
    "        # Draw landmarks\n",
    "        h, w, _ = frame.shape\n",
    "        annotated = frame.copy()\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated, hand_landmarks, mp.solutions.hands.HAND_CONNECTIONS\n",
    "            )\n",
    "    else:\n",
    "        annotated = frame.copy()\n",
    "\n",
    "    # -------------------------\n",
    "    # Feature collection\n",
    "    # -------------------------\n",
    "    if detected:\n",
    "        feature = np.array(left_hand + right_hand)\n",
    "        buffer.append(feature)\n",
    "    else:\n",
    "        buffer.clear()\n",
    "        last_pred = None\n",
    "\n",
    "    # -------------------------\n",
    "    # Prediction\n",
    "    # -------------------------\n",
    "    if len(buffer) == seq_len:\n",
    "        x = torch.tensor(np.array(buffer), dtype=torch.float32).unsqueeze(0)  # [1,30,126]\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            pred_id = torch.argmax(out, dim=1).item()\n",
    "            last_pred = classes[pred_id]\n",
    "\n",
    "    # -------------------------\n",
    "    # Overlay text\n",
    "    # -------------------------\n",
    "    import cv2  # only for text drawing\n",
    "    if last_pred is not None:\n",
    "        cv2.putText(annotated, f\"{last_pred}\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 255, 0), 4)\n",
    "    elif detected:\n",
    "        cv2.putText(annotated, \"Collecting frames...\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 165, 0), 4)\n",
    "    else:\n",
    "        cv2.putText(annotated, \"No hand detected\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "\n",
    "    # -------------------------\n",
    "    # Show inline in Jupyter\n",
    "    # -------------------------\n",
    "    plt.imshow(annotated)\n",
    "    plt.axis(\"off\")\n",
    "    clear_output(wait=True)\n",
    "    display(plt.gcf())\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3e5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
